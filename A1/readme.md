# Assignment 1: Finetuning a Pretrained Language Model

The goal of this assignment is to familiarize yourself with finetuning a pretrained language model using the huggingfaceâ€™s transformers library with GPUs available on CHPC. This skill is essential for the upcoming homeworks and project where the goal will be to apply explainability methods on such models â€“ if you cannot train a model, there is nothing to explain. ðŸ™‚

We ask you to finetune models on CHPC partly because resources such as Colabs do not provide consistent GPU access, but also because in your future roles as a AI researcher or engineer you will surely be expected to run jobs on clusters managed by Slurm and you wonâ€™t train them in notebooks.

The instructions are [here](https://docs.google.com/document/d/1NWmsqpm0EbwQ7lby-7vAm5PIchKkNQvCZNlwFJ5fqMc/edit?usp=sharing).

